# main.py - –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø
import os
import tempfile
import uuid
import asyncio
import logging
import sqlite3
from typing import Optional, List, Dict
from datetime import datetime

from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from dotenv import load_dotenv

# ‚ö†Ô∏è –ö–†–ò–¢–ò–ß–ù–û: –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv()

# –ò–º–ø–æ—Ä—Ç –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
try:
    from database import db
except ImportError:
    print("‚ö†Ô∏è  database.py –Ω–µ –Ω–∞–π–¥–µ–Ω. –°–æ–∑–¥–∞–π –µ–≥–æ –≤ –ø–∞–ø–∫–µ backend/")
    raise

# –ò–º–ø–æ—Ä—Ç Yandex —Å–µ—Ä–≤–∏—Å–æ–≤
try:
    from services.yandex_stt import transcribe_audio
    from services.yandex_gpt import analyze_transcript
    from services.gdocs_service import add_to_google_docs
    print("‚úÖ –ò–º–ø–æ—Ä—Ç —Å–µ—Ä–≤–∏—Å–æ–≤ —É—Å–ø–µ—à–µ–Ω")
except ImportError as e:
    print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ —Å–µ—Ä–≤–∏—Å–æ–≤: {e}")
    print("üìÅ –ü—Ä–æ–≤–µ—Ä—å –Ω–∞–ª–∏—á–∏–µ –ø–∞–ø–∫–∏ services/ –∏ —Ñ–∞–π–ª–æ–≤:")
    print("   - services/yandex_stt.py")
    print("   - services/yandex_gpt.py")
    print("   - services/gdocs_service.py")
    raise
# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
app = FastAPI(title="Voice2Action API v2.0")

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# In-memory —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –∑–∞–¥–∞—á
jobs: Dict[str, Dict] = {}


# ========== PYDANTIC –ú–û–î–ï–õ–ò ==========

class ExportRequest(BaseModel):
    job_id: str
    exports: List[str]


class JobStatus(BaseModel):
    job_id: str
    status: str
    progress: int
    complete: bool
    error: Optional[str] = None
    results: Optional[dict] = None


# ========== –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò ==========

async def process_pipeline(job_id: str, file_path: str, original_filename: str):
    """–ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
    try:
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ë–î
        db.create_job(job_id, original_filename)

        # 1. –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è
        logger.info(f"Job {job_id}: Starting transcription")
        transcript_data = await transcribe_audio(file_path)
        transcript_text = transcript_data["text"]

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç
        db.update_transcript(job_id, transcript_text)
        jobs[job_id]["transcript"] = transcript_text
        jobs[job_id]["transcript_chars"] = len(transcript_text)
        jobs[job_id]["progress"] = 50

        logger.info(f"Job {job_id}: Transcription completed ({len(transcript_text)} chars)")

        # 2. –ê–Ω–∞–ª–∏–∑ YandexGPT
        logger.info(f"Job {job_id}: Starting analysis")
        analysis_result = await analyze_transcript(transcript_text)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞–Ω–∞–ª–∏–∑
        db.save_analysis(job_id, analysis_result)
        jobs[job_id]["analysis"] = analysis_result
        jobs[job_id]["results"] = {
            "transcript": transcript_text,
            "summary": analysis_result.get("summary", ""),
            "tasks": analysis_result.get("tasks", []),
            "key_points": analysis_result.get("key_points", []),
            "decisions": analysis_result.get("decisions", [])
        }
        jobs[job_id]["progress"] = 75

        # 3. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ Google Docs
        logger.info(f"Job {job_id}: Saving to Google Docs")
        await add_to_google_docs(transcript_text, analysis_result)

        # –§–∏–Ω–∞–ª—å–Ω—ã–π —Å—Ç–∞—Ç—É—Å
        jobs[job_id]["status"] = "completed"
        jobs[job_id]["progress"] = 100
        jobs[job_id]["complete"] = True
        jobs[job_id]["completed_at"] = datetime.now().isoformat()

        logger.info(f"‚úÖ Job {job_id}: Pipeline completed")

    except Exception as e:
        logger.error(f"Job {job_id} failed: {e}")
        jobs[job_id]["status"] = "failed"
        jobs[job_id]["error"] = str(e)
        jobs[job_id]["complete"] = True

        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –≤ –ë–î
        conn = sqlite3.connect("voice2action.db")
        c = conn.cursor()
        c.execute('''UPDATE jobs SET status = ? WHERE id = ?''',
                  ("failed", job_id))
        conn.commit()
        conn.close()


# ========== API –≠–ù–î–ü–û–ò–ù–¢–´ ==========

@app.get("/")
async def root():
    return {
        "message": "Voice2Action API v2.0",
        "status": "running",
        "engine": "Yandex SpeechKit + YandexGPT",
        "docs": "/docs"
    }


@app.get("/health")
async def health_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Å–∏—Å—Ç–µ–º—ã"""
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î
        conn = sqlite3.connect("voice2action.db")
        c = conn.cursor()
        c.execute("SELECT COUNT(*) FROM jobs")
        db_count = c.fetchone()[0]
        conn.close()

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º Yandex –∫–ª—é—á–∏
        YANDEX_API_KEY = os.getenv("YANDEX_API_KEY")
        YANDEX_FOLDER_ID = os.getenv("YANDEX_FOLDER_ID")

        return {
            "status": "healthy",
            "timestamp": datetime.now().isoformat(),
            "services": {
                "fastapi": True,
                "database": True,
                "yandex_api": bool(YANDEX_API_KEY and len(YANDEX_API_KEY) > 10),
                "yandex_folder": bool(YANDEX_FOLDER_ID and len(YANDEX_FOLDER_ID) > 5),
                "google_docs": True
            },
            "statistics": {
                "total_jobs": db_count,
                "active_jobs": len([j for j in jobs.values() if j.get("status") == "processing"]),
                "completed_jobs": len([j for j in jobs.values() if j.get("status") == "completed"])
            }
        }
    except Exception as e:
        logger.error(f"Health check failed: {e}")
        return {
            "status": "degraded",
            "error": str(e),
            "services": {
                "fastapi": True,
                "database": False,
                "yandex_api": False,
                "google_docs": False
            }
        }


@app.get("/api/jobs", response_model=List[dict])
async def list_jobs(limit: int = 50, offset: int = 0):
    """–°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –∑–∞–¥–∞—á"""
    jobs_list = db.get_all_jobs(limit=limit)
    return [
        {
            "job_id": job["id"],
            "filename": job["filename"],
            "status": job["status"],
            "created_at": job["created_at"],
            "completed_at": job["completed_at"],
            "has_analysis": job.get("has_analysis", False)
        }
        for job in jobs_list[offset:offset + limit]
    ]


@app.get("/api/jobs/{job_id}", response_model=dict)
async def get_job_details(job_id: str):
    """–ü–æ–ª–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∑–∞–¥–∞—á–µ"""
    job_data = db.get_job(job_id)

    if not job_data:
        raise HTTPException(status_code=404, detail="Job not found")

    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
    response = {
        "job_id": job_data["id"],
        "filename": job_data["filename"],
        "status": job_data["status"],
        "created_at": job_data["created_at"],
        "completed_at": job_data["completed_at"],
        "transcript": {
            "text": job_data.get("transcript_text", ""),
            "characters": job_data.get("transcript_chars", 0)
        }
    }

    # –î–æ–±–∞–≤–ª—è–µ–º –∞–Ω–∞–ª–∏–∑ –µ—Å–ª–∏ –µ—Å—Ç—å
    if job_data.get("analysis"):
        response["analysis"] = job_data["analysis"]

    # –î–æ–±–∞–≤–ª—è–µ–º –∏–∑–≤–ª–µ—á—ë–Ω–Ω—ã–µ –∑–∞–¥–∞—á–∏
    if job_data.get("extracted_tasks"):
        response["extracted_tasks"] = job_data["extracted_tasks"]

    return response


@app.get("/api/jobs/{job_id}/transcript")
async def get_job_transcript(job_id: str):
    """–ü–æ–ª—É—á–∏—Ç—å —Ç–æ–ª—å–∫–æ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç"""
    job_data = db.get_job(job_id)

    if not job_data:
        raise HTTPException(status_code=404, detail="Job not found")

    return {
        "job_id": job_id,
        "transcript": job_data.get("transcript_text", ""),
        "characters": job_data.get("transcript_chars", 0)
    }


@app.get("/api/jobs/{job_id}/analysis")
async def get_job_analysis(job_id: str):
    """–ü–æ–ª—É—á–∏—Ç—å —Ç–æ–ª—å–∫–æ –∞–Ω–∞–ª–∏–∑"""
    job_data = db.get_job(job_id)

    if not job_data:
        raise HTTPException(status_code=404, detail="Job not found")

    if not job_data.get("analysis"):
        raise HTTPException(status_code=404, detail="Analysis not found")

    return {
        "job_id": job_id,
        "analysis": job_data["analysis"]
    }


@app.delete("/api/jobs/{job_id}")
async def delete_job(job_id: str):
    """–£–¥–∞–ª–∏—Ç—å –∑–∞–¥–∞—á—É"""
    if db.delete_job(job_id):
        # –¢–∞–∫–∂–µ —É–¥–∞–ª—è–µ–º –∏–∑ –ø–∞–º—è—Ç–∏
        if job_id in jobs:
            del jobs[job_id]
        return {"message": f"Job {job_id} deleted"}
    else:
        raise HTTPException(status_code=404, detail="Job not found")


@app.get("/api/stats")
async def get_statistics():
    """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"""
    stats = db.get_statistics()
    return {
        "statistics": stats,
        "timestamp": datetime.now().isoformat()
    }


@app.get("/api/search")
async def search_transcripts(query: str, limit: int = 20):
    """–ü–æ–∏—Å–∫ –ø–æ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞–º"""
    try:
        conn = sqlite3.connect("voice2action.db")
        conn.row_factory = sqlite3.Row
        c = conn.cursor()

        c.execute('''SELECT id, filename, transcript_text, created_at 
                    FROM jobs 
                    WHERE transcript_text LIKE ? 
                    AND transcript_text IS NOT NULL
                    ORDER BY created_at DESC 
                    LIMIT ?''',
                  (f"%{query}%", limit))

        results = []
        for row in c.fetchall():
            results.append({
                "job_id": row["id"],
                "filename": row["filename"],
                "snippet": row["transcript_text"][:200] + "..." if len(row["transcript_text"]) > 200 else row[
                    "transcript_text"],
                "created_at": row["created_at"]
            })

        conn.close()
        return {
            "query": query,
            "results": results,
            "count": len(results)
        }
    except Exception as e:
        logger.error(f"Search error: {e}")
        raise HTTPException(status_code=500, detail="Search failed")


@app.post("/api/process-audio")
async def process_audio(audio: UploadFile = File(...)):
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å –∞—É–¥–∏–æ –∏ –∑–∞–ø—É—Å—Ç–∏—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É"""
    try:
        # –°–æ–∑–¥–∞—ë–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID
        job_id = str(uuid.uuid4())

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª
        temp_dir = tempfile.gettempdir()
        safe_filename = f"{job_id}_{audio.filename.replace(' ', '_')}"
        file_path = os.path.join(temp_dir, safe_filename)

        with open(file_path, "wb") as f:
            content = await audio.read()
            f.write(content)

        # –°–æ–∑–¥–∞—ë–º –∑–∞–ø–∏—Å—å –≤ –ø–∞–º—è—Ç–∏
        jobs[job_id] = {
            "id": job_id,
            "filename": audio.filename,
            "status": "processing",
            "progress": 0,
            "complete": False,
            "created_at": datetime.now().isoformat(),
            "file_path": file_path
        }

        # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –≤ —Ñ–æ–Ω–µ
        asyncio.create_task(process_pipeline(job_id, file_path, audio.filename))

        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç
        return {
            "job_id": job_id,
            "message": "Processing started",
            "status_url": f"/api/jobs/{job_id}",
            "transcript_url": f"/api/jobs/{job_id}/transcript",
            "analysis_url": f"/api/jobs/{job_id}/analysis",
            "monitor_url": f"/api/status/{job_id}",
            "created_at": datetime.now().isoformat()
        }

    except Exception as e:
        logger.error(f"Upload error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/status/{job_id}")
async def get_status(job_id: str):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
    if job_id not in jobs:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤ –ë–î
        job_data = db.get_job(job_id)
        if job_data:
            return {
                "job_id": job_id,
                "status": job_data["status"],
                "progress": 100 if job_data["status"] in ["completed", "failed"] else 0,
                "complete": job_data["status"] in ["completed", "failed"],
                "filename": job_data["filename"],
                "created_at": job_data["created_at"],
                "completed_at": job_data.get("completed_at")
            }
        raise HTTPException(404, "Job not found")

    job = jobs[job_id].copy()
    job.pop("file_path", None)
    return job


@app.post("/api/export")
async def export_results(request: ExportRequest):
    """–≠–∫—Å–ø–æ—Ä—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ Google Docs/Sheets"""

    if request.job_id not in jobs:
        raise HTTPException(404, "Job not found")

    job = jobs[request.job_id]

    if not job.get("complete", False):
        raise HTTPException(400, "Job not completed yet")

    if job.get("error"):
        raise HTTPException(400, f"Job failed: {job['error']}")

    if not job.get("results"):
        raise HTTPException(400, "No results to export")

    exports = {}

    try:
        # Google Docs —ç–∫—Å–ø–æ—Ä—Ç (—á–µ—Ä–µ–∑ —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é —Ñ—É–Ω–∫—Ü–∏—é)
        if "google_docs" in request.exports:
            # –£–∂–µ —Å–¥–µ–ª–∞–Ω–æ –≤ process_pipeline —á–µ—Ä–µ–∑ add_to_google_docs
            exports["google_docs"] = {
                "status": "already_exported",
                "message": "Already exported during processing"
            }
            logger.info(f"Google Docs export noted for job {request.job_id}")

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π —ç–∫—Å–ø–æ—Ä—Ç –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –∑–¥–µ—Å—å

        return {"job_id": request.job_id, "exports": exports}

    except Exception as e:
        logger.error(f"Export failed for job {request.job_id}: {e}")
        raise HTTPException(500, f"Export failed: {str(e)}")


if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="0.0.0.0", port=8000)